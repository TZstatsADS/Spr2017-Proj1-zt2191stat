---
title: "Project 1"
output: html_notebook
    toc: true
    toc_depth: 2
---

# Step 0: check and install needed packages. Load the libraries and functions. 

```{r, message=FALSE, warning=FALSE}
packages.used=c("rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "tm", "topicmodels")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# load packages
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")

source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```



# Step 1: Data harvest: scrap speech URLs from <http://www.presidency.ucsb.edu/>.

Following the example of [Jerid Francom](http://francojc.github.io/web-scraping-with-rvest/), we used [Selectorgadget](http://selectorgadget.com/) to choose the links we would like to scrap. For this project, we selected all inaugural addresses of past presidents, nomination speeches of major party candidates and farewell addresses. We also included several public speeches from Donald Trump for our textual analysis of presidential speeches. 

```{r, message=FALSE, warning=FALSE}
### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches. 
inaug=f.speechlinks(main.page)
#head(inaug)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
```


# Step 2: Using speech metadata posted on <http://www.presidency.ucsb.edu/>, we prepared CSV data sets for the speeches we will scrap. 

```{r}
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
```

We assemble all scrapped speeches into one list. Note here that we don't have the full text yet, only the links to full text transcripts. 


# Step 3: scrap the texts of speeches from the speech URLs.

```{r}
speech.list=inaug.list
speech.list$type=rep("inaug", nrow(inaug.list))
speech.url=inaug
speech.list=cbind(speech.list, speech.url)
```


Based on the list of speeches, we scrap the main text part of the transcript's html page. For simple html pages of this kind,  [Selectorgadget](http://selectorgadget.com/) is very convenient for identifying the html node that `rvest` can use to scrap its content. For reproducibility, we also save our scrapped speeches into our local folder as individual speech files. 

```{r}
# Loop over each row in speech.list
speech.list$fulltext=NA
for(i in seq(nrow(speech.list))) {
  text <- read_html(speech.list$urls[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  speech.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/fulltext/", 
                     speech.list$type[i],
                     speech.list$File[i], "-", 
                     speech.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
}
```



```{r, warning= false}
library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)

speech.corpus <- Corpus(VectorSource(speech.list$fulltext))

speech.corpus <- tm_map(speech.corpus, stripWhitespace)
speech.corpus <- tm_map(speech.corpus, content_transformer(tolower))
speech.corpus <- tm_map(speech.corpus, removeWords, stopwords("english"))
speech.corpus <- tm_map(speech.corpus, removeWords, character(0))
speech.corpus <- tm_map(speech.corpus, removePunctuation)

tdm.all <- TermDocumentMatrix(speech.corpus)
tdm.tidy <- tidy(tdm.all)
tdm.overall <- summarise(group_by(tdm.tidy, term),sum(count))
```

Now we have all the texts we need, then we can do the text mining,

First let's try the Wordcloud.

```{r,fig.height=6, fig.width=6, message=FALSE}
wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
          scale=c(8,0.3),
          max.words=20,
          min.freq=10,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))
```

That is the general wordcloud for all inaugration speeches.




Let's try the word frequency.


```{r}
speech_tidy <- tidy(speech.corpus)
speech_tidy$author <- inaug.list$File
speech_token <- speech_tidy %>%
    unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word")
speech_token %>%
  count(word, sort = TRUE)
```

```{r}
freq_inaug <- speech_token %>%
    group_by(author) %>%
    count(word, sort = TRUE)%>%
    left_join(speech_token %>%
                  group_by(author) %>%
                  summarise(total = n())) %>%
    mutate(freq = n/total)
freq_inaug
```

That is what individual president uses words.

Then try again the wordcloud for some famous presidents.


```{r,fig.height=7.5, fig.width=7.5, message=FALSE}
wordcloud(freq_inaug$word[freq_inaug$author == "GeorgeWashington"], freq_inaug$n,
          scale=c(4.5,0.3),
          max.words=30,
          min.freq=10,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))

```


```{r,fig.height=7.5, fig.width=7.5, message=FALSE}
wordcloud(freq_inaug$word[freq_inaug$author == "AbrahamLincoln"], freq_inaug$n,
          scale=c(4.5,0.3),
          max.words=30,
          min.freq=10,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))

```



```{r,fig.height=7.5, fig.width=7.5, message=FALSE}
wordcloud(freq_inaug$word[freq_inaug$author == "JohnFKennedy"], freq_inaug$n,
          scale=c(4.5,0.3),
          max.words=30,
          min.freq=10,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))

```



```{r,fig.height=6.5, fig.width=6.5, message=FALSE}
wordcloud(freq_inaug$word[freq_inaug$author == "DonaldJTrump"], freq_inaug$n,
          scale=c(4.5,0.3),
          max.words=30,
          min.freq=10,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))

```









And I think from the wordclouds above, we can see that there are some differences in word using between each presidents, so I study the variation of the use of  each word. We could extract the year from each document's name, and compute the total number of words within each year.





```{r,fig.height=10, fig.width=10,message=false}
library(methods)
library(tidyr)
data("data_corpus_inaugural", package = "quanteda")
inaug_dfm <- quanteda::dfm(data_corpus_inaugural)

inaug_td <- tidy(inaug_dfm)

year_term_counts <- inaug_td %>%
  extract(document, "year", "(\\d+)", convert = TRUE) %>%
  complete(year, term, fill = list(count = 0)) %>%
  group_by(year) %>%
  mutate(year_total = sum(count))

year_term_counts %>%
  filter(term %in% c("will",  "can" , "public", "union", "america", "time","power","peace", "people")) %>%
  ggplot(aes(year, count / year_total)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~ term, scales = "free_y") +
  scale_y_continuous(labels = scales::percent_format()) +
  ylab("% frequency of word in inaugural address")
```

So we can see over time, the presidents are more likely to use "will" than "can", and less likely to refer to the country as the "Union" and more likely to refer to "America" and "nation". They also become more likely to talk about "time" and less in "peace". They keep using "people" while "public" is less likely to be referred.











